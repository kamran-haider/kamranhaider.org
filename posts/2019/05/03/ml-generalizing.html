<!DOCTYPE html>
<!--
    Basically Basic Jekyll Theme 1.1.3
    Copyright 2017 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/jekyll-basically-theme/blob/master/LICENSE.md
-->
<!-- MathJax/LaTeX support -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
    <title>Why generalizing beyond training data is so difficult?</title>
    <meta name="description" content="After a hiatus, I am back to jot down some notes on why machine learning algorithms struggle with generalizing beyond the training data.">
    <link rel="canonical" href="http://localhost:4000/posts/2019/05/03/ml-generalizing.html">
  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/stylesheets/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:400,400i,600,600i">
  

  
</head>


  <body class="layout--post why-generalizing-beyond-training-data-is-so-difficult">

    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>


    <div class="sidebar-toggle-wrapper">
      <button class="toggle navicon-button larr" type="button">
        <span class="toggle-inner">
          <span class="sidebar-toggle-label">Menu</span>
          <span class="navicon"></span>
        </span>
      </button>
    </div>

    <div id="sidebar" class="sidebar">
      <div class="inner">
        <nav id="nav-primary" class="site-nav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
  <ul id="menu-main-navigation" class="menu">
    <!-- Home link -->
    <li class="menu-item">
      <a href="/" itemprop="url">
        <span itemprop="name">Home</span>
      </a>
    </li>

    <!-- site.pages links -->
    
    

    
      
      
        <li class="menu-item">
          <a href="/about.html" itemprop="url">
            <span itemprop="name">About</span>
          </a>
        </li>
      
    
      
      
        <li class="menu-item">
          <a href="/cv.html" itemprop="url">
            <span itemprop="name">Resume</span>
          </a>
        </li>
      
    
      
      
        <li class="menu-item">
          <a href="/papers.html" itemprop="url">
            <span itemprop="name">Papers</span>
          </a>
        </li>
      
    
  </ul>
</nav>
        <ul class="contact-list">
  
    <li>
      <a href="mailto:kamranhaider.mb@gmail.com">
        <span class="icon icon--email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="313.1 3.7 16 16"><path d="M318.5 8.9c0-.2.2-.4.4-.4h4.5c.2 0 .4.2.4.4s-.2.4-.4.4h-4.5c-.3 0-.4-.2-.4-.4zm.4 2.1h4.5c.2 0 .4-.2.4-.4s-.2-.4-.4-.4h-4.5c-.2 0-.4.2-.4.4s.1.4.4.4zm3.5 1.2c0-.2-.2-.4-.4-.4h-3.1c-.2 0-.4.2-.4.4s.2.4.4.4h3.1c.2.1.4-.1.4-.4zm-1.5-8.4l-1.7 1.4c-.2.1-.2.4 0 .6s.4.2.6 0l1.4-1.2 1.4 1.2c.2.1.4.1.6 0s.1-.4 0-.6l-1.7-1.4c-.3-.1-.5-.1-.6 0zm7.8 6.2c.1.1.1.2.1.3v7.9c0 .8-.7 1.5-1.5 1.5h-12.5c-.8 0-1.5-.7-1.5-1.5v-7.9c0-.1.1-.2.1-.3l1.6-1.3c.2-.1.4-.1.6 0s.1.4 0 .6l-1.2 1 1.8 1.3v-4c0-.6.5-1.1 1.1-1.1h7.5c.6 0 1.1.5 1.1 1.1v4l1.8-1.3-1.2-1c-.2-.1-.2-.4 0-.6s.4-.2.6 0l1.6 1.3zm-11.6 2.2l4 2.8 4-2.8V7.6c0-.1-.1-.2-.2-.2h-7.5c-.1 0-.2.1-.2.2v4.6zm10.9-1l-4.7 3.4 3.4 2.6c.2.1.2.4.1.6-.1.2-.4.2-.6.1l-3.6-2.8-1.2.8c-.1.1-.3.1-.5 0l-1.2-.8-3.6 2.8c-.2.1-.4.1-.6-.1-.1-.2-.1-.4.1-.6l3.4-2.6-4.7-3.4v7.1c0 .4.3.6.6.6h12.5c.4 0 .6-.3.6-.6v-7.1z"/></svg></span>
        <span class="label">Email</span>
      </a>
    </li>
  

  
    <li><a href="https://github.com/kamran-haider">
  <span class="icon icon--github"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117 0 0 .67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147 0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48 0 1.07-.01 1.93-.01 2.19 0 .21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span>
  <span class="label">GitHub</span>
</a>
</li>
  

  
    <li><a href="https://twitter.com/kam_haider">
  <span class="icon icon--twitter"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812 0-3.282 1.47-3.282 3.28 0 .26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21 0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26 0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04 0 9.34-5 9.34-9.33 0-.14 0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span>
  <span class="label">Twitter</span>
</a>
</li>
  

  <li>
    
  </li>
</ul>

      </div>
    </div>

    <div class="canvas">
      <div class="wrapper">
        

<header id="masthead">
  <div class="inner">
    <div class="title-area">
      
        <p class="site-title">
          <a href="/">
            
            <span>Kamran Haider</span>
          </a>
        </p>
      
    </div>
  </div>
</header>

        <header class="intro">
  

  <div class="inner">
    <div class="intro-text">
      <h1 class="intro-title">Why generalizing beyond training data is so difficult?
</h1>
      

      
        

        <p class="entry-meta">
          <span class="byline-item">by Kamran Haider</span><span class="byline-item"><span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="379 72 16 16"><g><g><path fill="none" d="M380.8,86.7h12.3v-8.8h-12.3V86.7z M389.5,78.8h1.7v1.4h-1.7V78.8z M389.5,81.3h1.7v1.4h-1.7V81.3z M389.5,83.8h1.7v1.4h-1.7V83.8z M386.1,78.8h1.7v1.4h-1.7V78.8z M386.1,81.3h1.7v1.4h-1.7V81.3z M386.1,83.8h1.7v1.4h-1.7V83.8z M382.8,78.8h1.7v1.4h-1.7V78.8z M382.8,81.3h1.7v1.4h-1.7V81.3z M382.8,83.8h1.7v1.4h-1.7V83.8z"/><polygon fill="none" points="384.7 75.1 383.4 75.1 383.4 74.3 380.8 74.3 380.8 76.6 393.2 76.6 393.2 74.3 390.6 74.3 390.6 75.1 389.3 75.1 389.3 74.3 384.7 74.3"/><rect x="382.8" y="78.8" width="1.7" height="1.4"/><rect x="386.1" y="78.8" width="1.7" height="1.4"/><rect x="389.5" y="78.8" width="1.7" height="1.4"/><rect x="382.8" y="81.3" width="1.7" height="1.4"/><rect x="386.1" y="81.3" width="1.7" height="1.4"/><rect x="389.5" y="81.3" width="1.7" height="1.4"/><rect x="382.8" y="83.8" width="1.7" height="1.4"/><rect x="386.1" y="83.8" width="1.7" height="1.4"/><rect x="389.5" y="83.8" width="1.7" height="1.4"/><path d="M383.4,72v1.1h-3.8V88h14.9V73.1h-3.8V72h-1.3v1.1h-4.7V72H383.4z M393.2,86.7h-12.3v-8.8h12.3L393.2,86.7L393.2,86.7z M389.3,74.3v0.8h1.3v-0.8h2.5v2.3h-12.3v-2.3h2.5v0.8h1.3v-0.8H389.3z"/></g></g></svg></span><time datetime="2019-05-03T00:00:00-04:00">May 3, 2019</time></span> <span class="byline-item"><span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="15 309.7 16 16"><g><path d="M23.9 315.1v3.6c0 .5-.4.9-.9.9s-.9-.4-.9-.9v-3.6h1.8z"/><path d="M30.1 317.7c.5 3.9-2.3 7.5-6.2 7.9-3.9.5-7.5-2.3-7.9-6.2-.5-3.9 2.3-7.5 6.2-7.9v-1.8H24v1.8c1.1.1 2.7.7 3.5 1.4l1.3-1.3 1.3 1.3-1.3 1.3c.5.9 1.2 2.5 1.3 3.5zm-1.8.9c0-2.9-2.4-5.3-5.3-5.3s-5.3 2.4-5.3 5.3 2.4 5.3 5.3 5.3 5.3-2.3 5.3-5.3z"/></g></svg></span>13 min read</span>
        </p>
      

      

      
    </div>
  </div>
</header>

<main id="main" class="page-content" aria-label="Content">
  <div class="inner">
    <article class="entry-wrap">
      <div class="entry-content">
        <p>I recently came across a machine learning review <a href="http://physics.bu.edu/~pankajm/MLnotebooks.html"><em>A high-bias, low-variance introduction to Machine Learning for physicists</em></a>. I found it a pleasure to read for several reasons. It is really well-written and I find it quite accessible. It also comes with a set of great jupyter notebooks that illustrate key ideas through python code that readers can play with and do some exercises. These notebooks are available <a href="http://physics.bu.edu/~pankajm/MLnotebooks.html">online</a>. The review and corresponding notebooks are packed with great material. I decided to read it and comprehend as much as of it as possible by reproducing the notebooks. In this manner, I will be able to collect my notes and get practice the main ideas through coding. So here it goes, the first post on the topic of generalization in machine learning and why it is so hard.</p>

<h1 id="setting-the-stage">Setting the stage</h1>

<p>The setup for a typical Data Science or Machine Learning problem is as follows. We have a dataset, <script type="math/tex">\mathbf{X}</script>. We wish to fit a model <script type="math/tex">g(\mathbf{w})</script>, which is a function of paramaters <script type="math/tex">\mathbf{w}</script>. The process of fitting amounts to finding values of <script type="math/tex">\mathbf{w}</script> that minimize the cost function, <script type="math/tex">C(\mathbf{X}, g(\mathbf{w}))</script>. A key step in this approach is to split the data into two subsets, a training set <script type="math/tex">\mathbf{X}_{train}</script> and a test set <script type="math/tex">\mathbf{X}_{test}</script>.</p>

<p>The training set is used for model fitting and therefore  model parameters are given by:</p>

<script type="math/tex; mode=display">\;{\tiny\begin{matrix}
\\ \normalsize argmin 
\\ ^{\scriptsize \mathbf{w}}\end{matrix} }\;
\{C(\mathbf{X}_{train}, g(\mathbf{w}))\}</script>

<p>The value of the cost function for the training set <script type="math/tex">C(\mathbf{X}_{train}, g(\mathbf{w}))</script>  is called the training error or in-sample error, <script type="math/tex">E_{in}</script>. The value of the cost function for the test set <script type="math/tex">C(\mathbf{X}_{train}, g(\mathbf{w}))</script> is called the test error or out-of-sample error, <script type="math/tex">E_{out}</script>. In practice,  <script type="math/tex">\mathbf{X}_{train}</script> is actually dvided into <script type="math/tex">\mathbf{X}_{train}</script> and <script type="math/tex">\mathbf{X}_{validation}</script>, where the former is used for fitting and the later is used for hyperparameter tuning. We will ignore <script type="math/tex">\mathbf{X}_{validation}</script> for now. The out-of-sample error <script type="math/tex">E_{out}</script> is an unbiased measure of model performance, provided that test set is kept completely separate during model building, including steps involving feature scaling and selection. In most cases, we are unaware of the mathematical model that describes the data, and, therefore we make some assumptions and try a bunch of different models. The model that gives lowest <script type="math/tex">E_{out}</script> is selected and this is where the challenege begins. It turns out that the model with the lowest <script type="math/tex">E_{out}</script> is not necessarilty the one with the lowest <script type="math/tex">E_{in}</script>. Indeed, the model with the lowest <script type="math/tex">E_{in}</script> most likely has a poor performance on the the test set, because it is probably overfitting the training data.</p>

<p>This gap between <script type="math/tex">E_{in}</script> and <script type="math/tex">E_{out}</script> is at the heart of a major difficulty in machine learning. i.e., building models that generalize well to examples outside of training data. Here we will focus on some of the factors that influence this gap, namely the size of training data, noise in the measurement of training data and complexity of the underlying model. We will see that the noise leads to unreal trends in training data and when models are fit to such trends, the performance over test sets reduces significantly. These effects are even more pronounced with small amount of training data and/or with more complex models. An increase in the size of training data may drown out some of the noise and avoid ovefitting. Let’s get started by coding up some useful functions that will help us understand these ideas.</p>

<h2 id="generating-data">Generating Data</h2>

<p>We start by coding up a function to generate the data. This function mimics a process that generates data with intrinsic noise. For the purpose of this exercise, we can control the level of noise. We will also add an option for generating data either from a linear model, <script type="math/tex">f(x) = 2x</script> or from a non-linear model <script type="math/tex">H</script>, <script type="math/tex">f(x) = 2x - 10x^5 + 15x^{10}</script>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="n">mpl</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">"fivethirtyeight"</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">x_interval</span><span class="p">,</span> <span class="n">n_train</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"""
    Generate data in a given range of input values,  
    
    Parameters
    ----------
    x_interval : tuple
        A tuple containing the range in which the input data is generated.
    n_train : int
        The number of training examples.
    sigma : float
        Noise strength, this is the standard deviation of the Guassian noise.
    linear : bool
        If True, return data from a linear function, otherwise, return 
        data from a non-linear function.
        
    Returns
    -------
    x : numpy.ndarray
        Input values in the range defined by x_interval
    y : numpy.ndarray
        Labels for the input data generated from the model
    """</span>
    <span class="n">x_0</span><span class="p">,</span> <span class="n">x_n_train</span> <span class="o">=</span> <span class="n">x_interval</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_train</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_n_train</span><span class="p">,</span> <span class="n">n_train</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">linear</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">15</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">10</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span>
</code></pre></div></div>

<h2 id="building-models">Building Models</h2>
<p>Now we will code up a function that returns model(s) that are fit to the training data. The models returned from this function belong to the class of polynomial regression moodels. In order to use this function to generate polynomial regression models of degree &gt; 1, we have to first transform our data prior to using this function so that it contains polynomial features. For this purpose, we also code up the transformation function that takes our one-dimensional data as input and produces the transformation with additional polynomial features for the given degree, e.g., for degree 2, the transformation with create a feature matrix containing the original variable <script type="math/tex">x_i</script> and a new feature <script type="math/tex">x_i^2</script> for <script type="math/tex">i^{th}</script> training exmaple. As you can notice, the model fitting is performed through <code class="highlighter-rouge">scikit-learn</code> <code class="highlighter-rouge">LinearRegression</code> estimator using default arguments.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">"""
    Returns a polynomial regression model of degree upto but not more than 10. 
    
    Parameters
    ----------
    data : numpy.ndarray, shape = (n_train, order)
        Training data, it's assumed that data is already transformed for polynomials of order &gt;=1. 
    y : numpy.ndarray, shape(n_train,)
        Labels or target values.
    
    Returns
    -------
    model : sklearn.linear_model.LinearRegression
        A scikit-learn linear regression model object fit on the input data using default settings.  
    """</span>

    <span class="n">degree</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">degree</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Degree </span><span class="si">%</span><span class="s">d is not supported, try degree=&lt;</span><span class="si">%</span><span class="s">d"</span> <span class="o">%</span> <span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">transform_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">degree</span><span class="p">):</span>
    <span class="s">"""
    Transforms input data for polynomials of degree &gt;= 1.
    
    Parameters
    ----------
    data : numpy.ndarray, shape = (n_train, 1)
        Training data, in the shape of a column vector. 
    degree : int
        Order of the polynomial for which data needs to be transformed.
    
    Returns
    -------
    X : numpy.ndarray, shape = (n_train, degree)
        Transformed input data.
    """</span>

    <span class="k">if</span> <span class="n">degree</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">data</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">poly_features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">poly_features</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</code></pre></div></div>

<h2 id="analyzing-results">Analyzing Results</h2>
<p>We will run a bunch of experiments so let’s write a couple of utility functions that allow quick and easy comparisons.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="s">"""
    Returns mean squared error between target and predicted values.
    
    Parameters
    ----------
    y : numpy.ndarray
        Target values
    y_pred : numpy.ndarray
        Predicted values
    
    Returns
    -------
    ase : float
        Mean squared error between target and predicted values
    """</span>

    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mse</span>


<span class="k">def</span> <span class="nf">run_experiment</span><span class="p">(</span><span class="n">n_train</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                   <span class="n">x_interval_train</span><span class="o">=</span><span class="p">(</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">),</span> <span class="n">x_interval_test</span><span class="o">=</span><span class="p">(</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">1.40</span><span class="p">),</span>
                   <span class="n">linear</span><span class="o">=</span><span class="s">"True"</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="s">"""
    Returns results from a given experiment that specifies number and ranges of train and test data, 
    the type and level of noise in underlying data geenrating process. Each experiment is run with all
    three models under study.

    Parameters
    ==========
    n_train : int
        Number of training examples
    n_test : int
        Number of test examples
    x_interval_train : tuple
        The range of training data
    x_interval_test : tuple
        The range of test data
    noise : float
        Strength of noise in data generating porcess
    linear : bool
        If True the data generatiing process is linear, otherwise, non-linear.
    
    Returns
    =======
    inputs, results, errors : tuple
        inputs: list containing transformed train and test datasets
        results: a dictionary where each model class (linear, 3-degree or 10-degree polynonomial) 
        is a key and its value is a list of containing predictions on train and test data,e.g.,
        results["linear"] = [np.ndarry of training data predictions, np.ndarry of test data predictions].
        True labels are also stored under the key "target". 
        errors: a dictionary where each model class (linear, 3-degree or 10-degree polynonomial) 
        is a key and its value is a list containing errors on train and test data,e.g.,
        results["linear"] = [training error, test error].
    """</span>

    <span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"linear"</span><span class="p">,</span> <span class="s">"poly3"</span><span class="p">,</span> <span class="s">"poly10"</span><span class="p">]</span>
    <span class="n">model_degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="n">n</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">model_names</span> <span class="o">+</span> <span class="p">[</span><span class="s">"target"</span><span class="p">]}</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">{</span><span class="n">n</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">model_names</span><span class="p">}</span>
    
    
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">x_interval_train</span><span class="p">,</span> <span class="n">n_train</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="n">linear</span><span class="p">)</span>
    <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">x_interval_test</span><span class="p">,</span> <span class="n">n_test</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="n">linear</span><span class="p">)</span>
    
    <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="s">"target"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="s">"target"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>

    <span class="c1"># generate training and test data
</span>    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model_names</span><span class="p">):</span>
        <span class="n">degree</span> <span class="o">=</span> <span class="n">model_degrees</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="n">x_train_transform</span> <span class="o">=</span> <span class="n">transform_data</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
        <span class="n">x_test_transform</span> <span class="o">=</span> <span class="n">transform_data</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
        
        <span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">(</span><span class="n">x_train_transform</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train_transform</span><span class="p">)</span>
        <span class="n">e_in</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
        <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_transform</span><span class="p">)</span>
        <span class="n">e_out</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
        
        <span class="n">errors</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e_in</span><span class="p">)</span>
        <span class="n">errors</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e_out</span><span class="p">)</span>

        <span class="n">results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train_pred</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_test_pred</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">errors</span>


<span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">),</span> <span class="n">yticks</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="s">"""
    Generate plots of train and test data along with fitted functions. 
    """</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="s">'col'</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Training"</span><span class="p">,</span> <span class="s">"Test"</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="n">model_name</span><span class="p">][</span><span class="n">index</span><span class="p">]</span>
            <span class="n">marker</span> <span class="o">=</span> <span class="s">"-"</span>
            <span class="n">marker_size</span> <span class="o">=</span> <span class="mi">16</span>
            <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s">"target"</span><span class="p">:</span>
                <span class="n">marker</span> <span class="o">=</span> <span class="s">"o"</span>
                <span class="n">marker_size</span> <span class="o">=</span> <span class="mi">8</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="n">marker_size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">yticks</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
</code></pre></div></div>

<p>Please note that the function <code class="highlighter-rouge">run_experiment</code> hides away a lot of under-the-hood stuff so that we can focus on the results here. Just to refresh, it works by first generating some data from either linear or non-linear class and with or without the noise (depending on the arguments supplied at the call). Then it uses three models (linear, a degree 3 poylnomial and a degree 10 poylnomial). This is followed by testing the models on a separate test set. For each model, the training and test set errors are also recorded. The <code class="highlighter-rouge">plot_results</code> takes care of the plotting. All of this is meant to mimic what happens in a typical machine learning project; i.e.</p>
<ul>
  <li>We have data with features and target variables, it is split into training and test sets.</li>
  <li>Since we don’t know the true mathematical relationship, therefore, we chose a set of models to fit on the training data and then we use error on the test set to choose the best model.</li>
</ul>

<p>Now that everything is ready, let’s run some experiments.</p>

<h3 id="experiment-1">Experiment 1</h3>
<ul>
  <li>No noise in the data, <script type="math/tex">\sigma = 0</script></li>
  <li>Small training dataset, <script type="math/tex">m_{train} = 10</script></li>
  <li>The data is generated from the linear function.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">()</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training Error</span><span class="se">\t</span><span class="s">Test Error"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">errors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{0[0]:&lt;16.2f}{0[1]:.2f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">errors</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

<span class="n">Training</span> <span class="n">Error</span>	<span class="n">Test</span> <span class="n">Error</span>
<span class="mf">0.00</span>            <span class="mf">0.00</span>
<span class="mf">0.00</span>            <span class="mf">0.00</span>
<span class="mf">0.00</span>            <span class="mf">0.00</span>
</code></pre></div></div>
<center>
<figure>
<img src="/assets/ml-generalization-exp-01-56bf51aaa0230324c2b1c489dc4e9d96c62e43bc5140db3942ecdc4e508ad018.svg" />
</figure>
</center>

<p>This is the easiest situation. The simplest model easily captures the main trend in the data. Since, we are already at the lowest possible test error with the linear model, therefore, it isn’t necessary to increase the model complexity. Even if we do so, nothing really changes.</p>

<p>Let’s repeat the same experiment but this time let’s generate the underlying data from the non-linear function (which is a 10th degree polynomial).</p>

<p>Now we see that both training and test errors decrease as we increase model complexity. As soon as we fit a 10th degree polynomial, we are all set, no generalization error. In other words, in the noise-less case, our problem simply reduces to making sure that our model belongs to the same class as the data generating function.</p>

<h3 id="experiment-2">Experiment 2</h3>
<ul>
  <li>No noise in the data, <script type="math/tex">\sigma = 0</script></li>
  <li>Small training dataset, <script type="math/tex">m_{train} = 10</script></li>
  <li>Data generated from a 10th order polynomial (a non-linear process)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="n">linear</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training Error</span><span class="se">\t</span><span class="s">Test Error"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">errors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{0[0]:&lt;16.2f}{0[1]:.2f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">errors</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

<span class="n">Training</span> <span class="n">Error</span>	<span class="n">Test</span> <span class="n">Error</span>
<span class="mf">2.77</span>            <span class="mf">15430.96</span>
<span class="mf">0.64</span>            <span class="mf">12690.27</span>
<span class="mf">0.00</span>            <span class="mf">0.00</span>

</code></pre></div></div>
<center>
<figure>
<img src="/assets/ml-generalization-exp-02-2e6db603b31b7360141515ce5fd39aa8aeacbee03ffe3cf6990f4e76f2e9a9ab.svg" />
</figure>
</center>

<p>In this case, we still have no noise in the measurements but we begin with a model that is outside of the class that generates the model and increase complexity eventually matching up with the model that generated the data. As expexted, since the patterns in the data are real, the simpler models tend to ignore them and underfit. Underfitting is characterized by poor performance on both training and test sets, as seen in the table. As we increase the model complexity, we can reduce errors on both training and test sets. Just to reiterate, the reason why training and test set performances improve concomitantly with complex models and in the absence of noise is that any patterns in the training data can be assumed to be genuine and hence a complex model can account for such patterns.</p>

<p>Let’s move into the real world now and introduce some noise.</p>

<h3 id="experiment-3">Experiment 3</h3>
<ul>
  <li>Noise in the data, <script type="math/tex">\sigma = 1.0</script></li>
  <li>Relatively large training dataset, <script type="math/tex">m_{train} = 100</script></li>
  <li>Data generated from a linear model</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="n">n_train</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">),</span> <span class="n">yticks</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training Error</span><span class="se">\t</span><span class="s">Test Error"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">errors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{0[0]:&lt;16.2f}{0[1]:.2f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">errors</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

<span class="n">Training</span> <span class="n">Error</span>	<span class="n">Test</span> <span class="n">Error</span>
<span class="mf">0.89</span>            <span class="mf">1.26</span>
<span class="mf">0.86</span>            <span class="mf">4.40</span>
<span class="mf">0.74</span>            <span class="mf">82477701.06</span>
</code></pre></div></div>
<center>
<figure>
<img src="/assets/ml-generalization-exp-03-90c7e9be4c5a8b305cc2642ad298c7b6d813290625d370e062602951913f167e.svg" />
</figure>
</center>

<p>We are in serious trouble right away. The first thing to notice is the widening gap between training and test errors as we increase moel complexity. The model with the lowest training error has the worst test error. The plot on the right shows clearly where these errors comes from i.e., test cases outside the range of training data. The training error decreases as we build more complex models because the model fit to the noisy patterns in the data. But the same model is completely off as soon as we step outside the training data range. Strikingly, the linear model, which is the simplest of the three, generalizes quite well. But we should also note that the underlying model that generated the data also belongs to the same class. In other words, the choice of correct model class with limited training data leads to much better generalization.</p>

<p>Unfortunately, we are never truly aware of the mathematical model that generates the data. Let’s see that in the  next experiment a bit more concretely.</p>

<h3 id="experiment-4">Experiment 4</h3>
<ul>
  <li>Noise in the data, <script type="math/tex">\sigma = 1.0</script></li>
  <li>Relatively large training dataset, <script type="math/tex">m_{train} = 100</script></li>
  <li>Data generated from a non-linear model</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="n">n_train</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">),</span> <span class="n">yticks</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training</span><span class="se">\t</span><span class="s">Test"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">errors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{0[0]:&lt;16.2f}{0[1]:.2f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">errors</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

<span class="n">Training</span>	<span class="n">Test</span>
<span class="mf">1.92</span>            <span class="mf">10315.88</span>
<span class="mf">1.28</span>            <span class="mf">8977.04</span>
<span class="mf">0.92</span>            <span class="mf">213488.34</span>
</code></pre></div></div>
<center>
<figure>
<img src="/assets/ml-generalization-exp-04-17603008d414f6b875b5903cb37934bea6260b73a7221c9bbababae0aa75888e.svg" />
</figure>
</center>

<p>The data comes from a non-linear model and still has noise in it, so we have a mix of real and unreal trends in the data. We want the model to learn the major trends but ignore the noisy fluctuations.</p>

<p>We begin with a model outside of this class, i.e., a linear model. Unfortunately, it misses out on a key trend at the higher values of <script type="math/tex">x</script>, therefore, it performs quite badly on the test set. We increase the model complexity a bit and notice a simultaneous improvement on training and test set performance. Encouraged by this, we fit a more complex model. The training error goes down, but suprisingly the test error goes up significantly. We starting to see something interesting here, a possible sweet spot where trainig error isn’t the minimum but the test error is! This will eventually lead us to the notion of bias variance trade-off, which we haven’t discussed in detail in thise notebook. For now let’s see what happens if we increase the size of training data. The question here is that can we gain test set performance by using complex model on a large dataset.</p>

<h3 id="experiment-5">Experiment 5</h3>
<ul>
  <li>Noise in the data, <script type="math/tex">\sigma = 1</script></li>
  <li>Relatively large training dataset, <script type="math/tex">m_{train} = 10000</script></li>
  <li>Data generated from a non-linear model</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="n">n_train</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_test</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plot_results</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">),</span> <span class="n">yticks</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training Error</span><span class="se">\t</span><span class="s">Test Error"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">errors</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{0[0]:&lt;16.2f}{0[1]:.2f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">errors</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

<span class="n">Training</span> <span class="n">Error</span>	<span class="n">Test</span> <span class="n">Error</span>
<span class="mf">1.99</span>            <span class="mf">7051.25</span>
<span class="mf">1.36</span>            <span class="mf">5910.58</span>
<span class="mf">1.02</span>            <span class="mf">979.75</span>
</code></pre></div></div>
<center>
<figure>
<img src="/assets/ml-generalization-exp-05-7d14e55b934fbd9310c2c763faf49d61724532fa5c61ad1ae04029e3f30f63b2.svg" />
</figure>
</center>

<p>It seems that this strategy may be working. Both the training and test error goes down with increasing model complexity. So increasing the size of the training data is certainly useful here because generalization error is lowest for the most complex model.</p>

<h1 id="concluding-remarks">Concluding Remarks</h1>
<p>The experiments above indicate that with the limited nosiy training data, we can easily overfit and have a poor predictive performance on the new data. A simple model may give better performance in such cases. It would be less sensitive to the training data used. It will certainly not capture the true relationship (i.e., it will have high bias) but at the same time, a more complex model would be highly sensitive to the particular realization of the training dataset (see this in action by simply repeating experiment 3 without even changing anything). In other words, the model will have high variance.</p>


      </div>
      
    </article>

    <footer id="footer" class="site-footer">
  <large><a href="http://localhost:4000">Home</a></large>
  <div class="copyright">
    
      <p>&copy; 2019 Kamran Haider</p>
    
  </div>
</footer>

  </div>
</main>

      </div>
    </div>

    

<script async src="/assets/javascripts/main.js"></script>

  </body>

</html>
