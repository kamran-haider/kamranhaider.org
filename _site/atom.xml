<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2019-05-03T10:26:35-04:00</updated><id>http://localhost:4000/</id><title type="html">Kamran Haider</title><subtitle>Ensemble of Thoughts</subtitle><author><name>Kamran Haider</name></author><entry><title type="html"></title><link href="http://localhost:4000/2019/05/03/2017-12-28-isingpy.html" rel="alternate" type="text/html" title="" /><published>2019-05-03T10:26:35-04:00</published><updated>2019-05-03T10:26:35-04:00</updated><id>http://localhost:4000/2019/05/03/2017-12-28-isingpy</id><content type="html" xml:base="http://localhost:4000/2019/05/03/2017-12-28-isingpy.html">&lt;p&gt;IsingPy began as a fun project to use the famous Ising model to learn and implement statistical physics algrithms.
My goals in this project are twofold:&lt;/p&gt;

&lt;p&gt;1\ Develop efficient implementations of Monte Carlo algorithms to sample spin states and calculate macroscopic observables, such as magnetization and average energy. This is more of a textbook style approach in learning statsitcal physics with Ising model.&lt;/p&gt;

&lt;p&gt;2\ In a more ambitious attempt at a later stage, I wish to explore the crossover of statistical physics and machine learning
using Ising model, e.g., building generative models that sample spin states, using deep learning to solve inverse ising inference problem.&lt;/p&gt;

&lt;p&gt;So far, I have managed to write the basic &lt;a href=&quot;https://github.com/kamran-haider/isingpy/blob/master/examples/modeling_ising_magnets_first_attempt.ipynb&quot;&gt;code&lt;/a&gt; 
that simulates Ising models of moderate sizes and reproduces the phase 
transition at the critical temperature. Currently, I am working to implement more advanced approaches, 
such as cluster algorithms, as outlined in Chapter 5 of 
&lt;a href=&quot;https://global.oup.com/academic/product/statistical-mechanics-9780198515364?cc=us&amp;amp;lang=en&amp;amp;&quot;&gt;Statistical Mechanics Algorithms and Computations&lt;/a&gt; 
by Werner Krauth.&lt;/p&gt;

&lt;p&gt;For goal 2, I am waiting to go through the convolutional neural networks course on the &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Coursera Deep Learning Specialization&lt;/a&gt;, 
which I have signed up for. I am hoping that this course will equip me with tools to build and train appropriate deep learning
models for Ising systems to solve problems such as &lt;a href=&quot;https://www.nature.com/articles/nphys4035&quot;&gt;learning phase transitions&lt;/a&gt; 
and &lt;a href=&quot;https://arxiv.org/abs/1706.08466&quot;&gt;inverse Ising inference&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kamran Haider</name></author></entry><entry><title type="html">Coding up a Toy Deep Neural Network</title><link href="http://localhost:4000/posts/2018/02/15/coding-up-neural-networks.html" rel="alternate" type="text/html" title="Coding up a Toy Deep Neural Network" /><published>2018-02-15T00:00:00-05:00</published><updated>2018-02-15T00:00:00-05:00</updated><id>http://localhost:4000/posts/2018/02/15/coding-up-neural-networks</id><content type="html" xml:base="http://localhost:4000/posts/2018/02/15/coding-up-neural-networks.html">&lt;p&gt;My interest in deep learning began after having conversations with &lt;a href=&quot;http://rbharath.github.io/&quot;&gt;Bharath Ramsundar&lt;/a&gt; 
(lead developer of &lt;a href=&quot;https://deepchem.io/&quot;&gt;DeepChem&lt;/a&gt;) and &lt;a href=&quot;https://research.google.com/pubs/StevenKearnes.html&quot;&gt;Steven Kearnes&lt;/a&gt; 
(a Google researcher working on applications of deep learning in drug discovery), last summer at 2017 Gordon Computer-aided 
Drug Design Conference. I was impressed by their work and by the fact that deep learning neural networks are remarkably 
successful in computing complex functions. In my own work, the ability to approximate complex functions is a routine part of the job, 
therefore, I was naturally interested.&lt;/p&gt;

&lt;p&gt;Within minutes of googling deep learning, I noticed how amazingly easy it is to get started in this area. If you have 
basic programming and maths skills, the technical barrier is quite low. There are great libraries and tools, such as 
&lt;a href=&quot;https://keras.io/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TensorFlow&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Theano&lt;/code&gt;&lt;/a&gt;, 
&lt;a href=&quot;http://pytorch.org/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PyTorch&lt;/code&gt;&lt;/a&gt;, and many others that get you started in building and training models very quickly. 
There is also an amazing amount of learning material available online in the form of &lt;a href=&quot;https://www.deeplearning.ai/&quot;&gt;courses&lt;/a&gt;, 
&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/index.html&quot;&gt;books&lt;/a&gt;, and &lt;a href=&quot;http://colah.github.io/&quot;&gt;blogs&lt;/a&gt;. 
I chose Andrew Ng’s &lt;a href=&quot;https://www.deeplearning.ai/&quot;&gt;Coursera deep learning specialization&lt;/a&gt; and Michael Nielsen’s online 
&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/index.html&quot;&gt;book&lt;/a&gt; as starting points.&lt;/p&gt;

&lt;p&gt;To gain a better understanding of deep learning, especially the algorithmic aspects, I decided to spend 
some time coding up a simple &lt;a href=&quot;https://github.com/kamran-haider/toynn&quot;&gt;implementation&lt;/a&gt; of 
neural networks (named &lt;code class=&quot;highlighter-rouge&quot;&gt;toynn&lt;/code&gt;) based on whatever I have learnt so far. Obviously, there are infinitely better 
implementations available in the tools that I mentioned above. However, the motivation behind creating &lt;code class=&quot;highlighter-rouge&quot;&gt;toynn&lt;/code&gt; was 
to get a better understanding of how basic deep neural networks really work.&lt;/p&gt;

&lt;p&gt;As I was coding this up, I was thinking about the idea of learning by doing. I would digress a little bit just to share an anecdote. 
I was visiting a collaborator at University of Cambridge in December 2012 and got lucky to catch a talk by the great 
&lt;a href=&quot;https://www.bakerlab.org/&quot;&gt;David Baker&lt;/a&gt;, one of my most favorite scientists and one of the top researchers in the field of
protein design. He said, while talking about the motivation  behind designing proteins in the laboratory, 
&lt;strong&gt;“We don’t know much about proteins, so we thought we should just create them 
to get a better understanding.”&lt;/strong&gt; His nonchalance felt even more impressive after he gave a fascinating talk on his recent work. 
I thought this must be an empowering feeling, i.e., create things you want to understand. This also reminds me of what Demis
Hassabis of &lt;a href=&quot;https://deepmind.com/&quot;&gt;DeepMind&lt;/a&gt; said in one of his talks, &lt;strong&gt;“The ultimate expression of understanding something is to be able to recreate it.”&lt;/strong&gt;, 
which he points out comes from Richard Feynman’s words that &lt;strong&gt;“What I cannot build, I do not understand”&lt;/strong&gt;.
Taking inspiration from this, I have tried to use this idea as a general principle to understand 
concepts. Obviously, it applies only in cases where there is a tractable way of creating something that directly or indirectly
sheds ligt on the concept I am trying to learn. That’s why despite the abundance of great deep learning libraries, I wanted to 
code up a neural network from scratch.&lt;/p&gt;

&lt;p&gt;I wouldn’t go ahead and give an introduction to deep learning or neural networks here and rather point readers to the 
&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;first chapter&lt;/a&gt; of Michael Nielsen’s book. Here, I will just 
add a couple  of notes about my implementation.&lt;/p&gt;

&lt;p&gt;I am a huge fan of &lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt; estimator API, which closely follows how machine learning projects are structured in general. 
Once the data has been adequately pre-processed, a machine learning task can be done with the following steps in &lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choosing a model&lt;/li&gt;
  &lt;li&gt;Fitting model to the data with &lt;code class=&quot;highlighter-rouge&quot;&gt;fit()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Applying the trained model to new data with &lt;code class=&quot;highlighter-rouge&quot;&gt;predict()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Refine and fine-tune the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following this API, I created a module called &lt;code class=&quot;highlighter-rouge&quot;&gt;models&lt;/code&gt; that consists of different types of neural networks that 
are supported. Currently, only one type is implemented which is called, &lt;code class=&quot;highlighter-rouge&quot;&gt;BasicDeepModel&lt;/code&gt;. 
As you’d have guessed, each type of model has &lt;code class=&quot;highlighter-rouge&quot;&gt;fit()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;predict()&lt;/code&gt; functions. The &lt;code class=&quot;highlighter-rouge&quot;&gt;BasicDeepModel&lt;/code&gt; itself is built from 
layers, whcih are implemented in a separate module &lt;code class=&quot;highlighter-rouge&quot;&gt;layers&lt;/code&gt;. Currently, the distinction between layers is based on the non-linearities that 
are used to calculate activations of the constituent nodes. For example, two layers are supported; &lt;code class=&quot;highlighter-rouge&quot;&gt;Sigmoid&lt;/code&gt; and 
&lt;code class=&quot;highlighter-rouge&quot;&gt;ReLU&lt;/code&gt;, which use sigmoid and ReLU activation functions, respectively. To maintain consistency in the design, an &lt;code class=&quot;highlighter-rouge&quot;&gt;Input&lt;/code&gt; 
layer is also implemented whose activations are initialized from the neural network inputs. The forward and backward methods 
for input layer don’t really do anything. However, coding the inputs as a layers class allows me to write compact 
forward and backward propagation methods for the network. Finally, the &lt;code class=&quot;highlighter-rouge&quot;&gt;utils&lt;/code&gt; module contains some useful functions,
such as loss functions, their derivatives, and network parameter initialization schemes.&lt;/p&gt;

&lt;p&gt;Now, let’s see all of this in action. An example workflow for a binary classification problem would look like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;toynn.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BasicDeepModel&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;toynn.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;toynn.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;../toynn/tests/test_datasets/train_catvnoncat.h5&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;../toynn/tests/test_datasets/test_catvnoncat.h5&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_x_orig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x_orig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_px&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_x_orig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Pre-processing of data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_x_flatten&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_x_orig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x_orig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_x_flatten&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x_orig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_x_orig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_x_flatten&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;255.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x_flatten&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;255.&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;input_layer_nodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_px&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_px&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_layer_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BasicDeepModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_initialization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;custom&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0075&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;predictions&lt;/code&gt; is an array consisting of the probability of belonging to the class for each data point. 
One can easily check the accuracy by converting these probabilities to class labels and then comparing with labels in 
&lt;code class=&quot;highlighter-rouge&quot;&gt;test_y&lt;/code&gt;. See full example in a jupyter notebook &lt;a href=&quot;https://github.com/kamran-haider/toyNN/blob/master/examples/01-recognizing-cat-images.ipynb&quot;&gt;here&lt;/a&gt;.
My starting point for this implementation was the material from Week 4 of the first course in Coursera deep learning specialization. 
I have checked the implementation using a dataset of cat images and reproduced the test accuracy of 0.8, which is identical 
to the implementation provided in the course. I also drew inspiration from another great and more comprehensive implementation
I found &lt;a href=&quot;https://github.com/cstorm125/sophia/blob/master/from_scratch.ipynb&quot;&gt;here&lt;/a&gt;. Isn’t it amazing that we are living 
in a world where people do cool stuff on Jupyter notebooks and then make it accessible to everyone?&lt;/p&gt;

&lt;p&gt;The most fun part of coding up a neural network from scratch was to see backpropagation unravel as a set of matrix multiplications.
There is definitely a lot of room for improvement and enhancements in &lt;code class=&quot;highlighter-rouge&quot;&gt;toynn&lt;/code&gt;, such as better documentation, unit tests and 
features such as regularization and advanced architectures. For now, I would continue diving a bit deeper into how neural 
networks can be tuned to solve various problems using some of the existing amazing tools, such as 
&lt;code class=&quot;highlighter-rouge&quot;&gt;PyTorch&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Keras&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorFlow&lt;/code&gt;. Nevertheless, I am glad that I have &lt;code class=&quot;highlighter-rouge&quot;&gt;toynn&lt;/code&gt; to play with whenever 
I needed to understand something through coding. I was also asked by at least one colleague that they 
would like to take a look at it and may be tear it apart and rebuild it to learn how it works. 
So I thought it would be a good idea to share it with everyone. 
If you are reading this and feel intrigued, feel free to take a look at the 
&lt;a href=&quot;https://github.com/kamran-haider/toynn&quot;&gt;code&lt;/a&gt;, provide feedback or use it for your own practice.&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="posts" /><category term="Python" /><category term="Deep Learning" /><summary type="html">Learning by doing is the best. I recently attempted implementing a [bare-bones deep neural network](https://github.com/kamran-haider/bbbp_ml_study/tree/master/code/toyNN) from scratch and it was so much fun.</summary></entry><entry><title type="html">Modeling Proton Hopping</title><link href="http://localhost:4000/projects/2017/12/28/proton-hopping.html" rel="alternate" type="text/html" title="Modeling Proton Hopping" /><published>2017-12-28T00:00:00-05:00</published><updated>2017-12-28T00:00:00-05:00</updated><id>http://localhost:4000/projects/2017/12/28/proton-hopping</id><content type="html" xml:base="http://localhost:4000/projects/2017/12/28/proton-hopping.html">&lt;p&gt;If not for proton-hopping, we wouldn’t be alive. Indeed, a crucial step in aerobic respiration is the proton-coupled
electron transport at the mitochondrial membrane. The proteins that facilitate this process take up protons from the 
surroundings and move them across membranes. The study of proton diffusion in aqueous media and across proteins is a 
major area of research in biochemistry and molecular biophysics and also a focus of research at 
&lt;a href=&quot;http://www.sci.ccny.cuny.edu/~gunner/&quot;&gt;Gunner Lab&lt;/a&gt;, where I work. My main project here is to calculate energetics of 
proton-hopping in relatively simple systems (such as Gramicidin channel) and in proteins such as Cytochrome c Oxidase (CcO). 
The main tool in my kit is the Multi-conformer Continuum Electrostatics (MCCE) approach, which is essentially a 
Monte Carlo simulation method to model protontation state changes in biomolecules. We wish to develop an extension of 
MCCE to evaluate proton transfer paths in membrane spanning proteins like CcO to develop a complete picture of how 
protons move from one side of the membrane to the other.&lt;/p&gt;

&lt;p&gt;I am in the process of organizing the data and code from this project into a robust and reproducible workflow. The &lt;a href=&quot;https://github.com/kamran-haider/ProtonHopping&quot;&gt;project repository&lt;/a&gt; is up but is being updated frequently. The notebooks subdirectory will contain key results and analysis.&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="projects" /><summary type="html">If not for proton-hopping, we wouldn’t be alive. Indeed, a crucial step in aerobic respiration is the proton-coupled electron transport at the mitochondrial membrane. The proteins that facilitate this process take up protons from the surroundings and move them across membranes. The study of proton diffusion in aqueous media and across proteins is a major area of research in biochemistry and molecular biophysics and also a focus of research at Gunner Lab, where I work. My main project here is to calculate energetics of proton-hopping in relatively simple systems (such as Gramicidin channel) and in proteins such as Cytochrome c Oxidase (CcO). The main tool in my kit is the Multi-conformer Continuum Electrostatics (MCCE) approach, which is essentially a Monte Carlo simulation method to model protontation state changes in biomolecules. We wish to develop an extension of MCCE to evaluate proton transfer paths in membrane spanning proteins like CcO to develop a complete picture of how protons move from one side of the membrane to the other.</summary></entry><entry><title type="html">Machine Learning for Predicting Blood Brain Barrier Crossing</title><link href="http://localhost:4000/projects/2017/12/27/bbbp-ml.html" rel="alternate" type="text/html" title="Machine Learning for Predicting Blood Brain Barrier Crossing" /><published>2017-12-27T00:00:00-05:00</published><updated>2017-12-27T00:00:00-05:00</updated><id>http://localhost:4000/projects/2017/12/27/bbbp-ml</id><content type="html" xml:base="http://localhost:4000/projects/2017/12/27/bbbp-ml.html">&lt;p&gt;I recently started working on an independent machine learning project, partly inspired by the idea
that understanding machine learning requires having a problem to work on. The development of drugs that target central 
nervous system requires crossing the blood-brain barrier, a natural mechanism for the brain to prevent
chemicals with certain properties from barging in. In a drug discovery campaign for a CNS traget, it is worthwhile knowing 
eraly on if the promising compounds can cross this barrier. 
That’s where machine-learning can come in handy. My current goals are:&lt;/p&gt;

&lt;p&gt;1/ Reproduce &lt;a href=&quot;http://http://moleculenet.ai/&quot;&gt;MoleculeNet&lt;/a&gt; benchmarks for the performance of a diverse set of machine learning algorithms on the 
Blood-brain barrier prediction &lt;a href=&quot;http://pubs.acs.org/doi/abs/10.1021/ci300124c&quot;&gt;dataset&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;2/ Diagnose issues leading to poor model performance on test data and develop strategies to improve performance.&lt;/p&gt;

&lt;p&gt;In the long term, I am also interested in evaluating if deep learning neural networks can give a significant performance boost. 
The best place to see current progress is to directly go to the &lt;a href=&quot;https://github.com/kamran-haider/bbbp_ml_study/tree/master/notes&quot;&gt;notes&lt;/a&gt; 
or &lt;a href=&quot;https://github.com/kamran-haider/bbbp_ml_study/tree/master/scratch&quot;&gt;scratch&lt;/a&gt; subdirecties of the project 
&lt;a href=&quot;https://github.com/kamran-haider/bbbp_ml_study&quot;&gt;Github repository&lt;/a&gt;. From time to time, I will post about the ongoing 
progress and results on the &lt;a href=&quot;/home&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="projects" /><summary type="html">I recently started working on an independent machine learning project, partly inspired by the idea that understanding machine learning requires having a problem to work on. The development of drugs that target central nervous system requires crossing the blood-brain barrier, a natural mechanism for the brain to prevent chemicals with certain properties from barging in. In a drug discovery campaign for a CNS traget, it is worthwhile knowing eraly on if the promising compounds can cross this barrier. That’s where machine-learning can come in handy. My current goals are:</summary></entry><entry><title type="html">Developing and Maintaining SSTMap</title><link href="http://localhost:4000/projects/2017/12/27/SSTMap.html" rel="alternate" type="text/html" title="Developing and Maintaining SSTMap" /><published>2017-12-27T00:00:00-05:00</published><updated>2017-12-27T00:00:00-05:00</updated><id>http://localhost:4000/projects/2017/12/27/SSTMap</id><content type="html" xml:base="http://localhost:4000/projects/2017/12/27/SSTMap.html">&lt;p&gt;I continue developing and maintaining SSTMap, which is my first complete software project. 
SSTMap is a valuable addition to an expanding set of software packages for the analysis of water molecules in 
molecular dynamics trajectories. Its main advantages are its applicability across multiple MD platforms and flexibility in
carrying out calculations in different forms (e.g., grid-based vs cluster-based, choosing all or a subset of energy, entropy
and hydrogen bonding calculations). There is still plenty of room for improvement in its API and efficiency, 
which I am working on. Stay tuned for more updates on the &lt;a href=&quot;http://sstmap.org&quot;&gt;project website&lt;/a&gt;&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="projects" /><summary type="html">I continue developing and maintaining SSTMap, which is my first complete software project. SSTMap is a valuable addition to an expanding set of software packages for the analysis of water molecules in molecular dynamics trajectories. Its main advantages are its applicability across multiple MD platforms and flexibility in carrying out calculations in different forms (e.g., grid-based vs cluster-based, choosing all or a subset of energy, entropy and hydrogen bonding calculations). There is still plenty of room for improvement in its API and efficiency, which I am working on. Stay tuned for more updates on the project website</summary></entry><entry><title type="html">Solvation Structure and Thermodynamic Mapping</title><link href="http://localhost:4000/papers/2017/11/21/SSTMap.html" rel="alternate" type="text/html" title="Solvation Structure and Thermodynamic Mapping" /><published>2017-11-21T00:00:00-05:00</published><updated>2017-11-21T00:00:00-05:00</updated><id>http://localhost:4000/papers/2017/11/21/SSTMap</id><content type="html" xml:base="http://localhost:4000/papers/2017/11/21/SSTMap.html">&lt;p&gt;We describe the implementation and features of &lt;a href=&quot;http://sstmap.org&quot;&gt;SSTMap&lt;/a&gt;, an interoperable package for the analysis of water molecules in molecualr dynamics simulations of biomolecules.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Haider K&lt;/strong&gt;, Cruz A, Ramsey S, Gilson M. and Kurtzman T. Solvation Structure and Thermodynamic Mapping (SSTMap): An Open-Source, Flexible Package for the Analysis of Water in Molecular Dynamics Trajectories. &lt;a href=&quot;http://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00592&quot;&gt;J. Chem. Theory Comput. 2017&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="papers" /><summary type="html">We describe the implementation and features of SSTMap, an interoperable package for the analysis of water molecules in molecualr dynamics simulations of biomolecules.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22thumb%22=%3E%22sstmap.png%22%7D" /></entry><entry><title type="html">Predicting Host-Guest Binding in the SAMPL5 Challenge</title><link href="http://localhost:4000/papers/2016/09/30/AGBNP2-HSA.html" rel="alternate" type="text/html" title="Predicting Host-Guest Binding in the SAMPL5 Challenge" /><published>2016-09-30T00:00:00-04:00</published><updated>2016-09-30T00:00:00-04:00</updated><id>http://localhost:4000/papers/2016/09/30/AGBNP2-HSA</id><content type="html" xml:base="http://localhost:4000/papers/2016/09/30/AGBNP2-HSA.html">&lt;p&gt;As part of a collaboration between &lt;a href=&quot;http://www.lehman.edu/faculty/tkurtzman/&quot;&gt;Kurtzman Lab&lt;/a&gt;, &lt;a href=&quot;https://ronlevygroup.cst.temple.edu/&quot;&gt;Levy Group&lt;/a&gt; and &lt;a href=&quot;http://www.compmolbiophysbc.org/&quot;&gt;Gallicchio Lab&lt;/a&gt;, we headed to SAMPL5-blinded challenge for predicting host-guest binding affinities. This work employed BEDAM absolute free energy calculations with an implicit solvent model parameterized by IFST-based hydration site analysis. After including a Debye–Hückel treatment of salt effects, calculations were in good agreement with the experimental results.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Pal R K, &lt;strong&gt;Haider K&lt;/strong&gt;, Kaur D, Flynn W, Xia J, Levy R M, Taran T, Wickstrom L, Kurtzman T, Gallicchio E. A combined treatment of hydration and dynamical effects for the modeling of host–guest binding thermodynamics: the SAMPL5 blinded challenge. &lt;a href=&quot;https://link.springer.com/article/10.1007/s10822-016-9956-6&quot;&gt;JCAMD. 2016&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="papers" /><summary type="html">As part of a collaboration between Kurtzman Lab, Levy Group and Gallicchio Lab, we headed to SAMPL5-blinded challenge for predicting host-guest binding affinities. This work employed BEDAM absolute free energy calculations with an implicit solvent model parameterized by IFST-based hydration site analysis. After including a Debye–Hückel treatment of salt effects, calculations were in good agreement with the experimental results.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22thumb%22=%3E%22sampl5-study.png%22%7D" /></entry><entry><title type="html">Enthalpic Breakdown of Water Structure</title><link href="http://localhost:4000/papers/2016/05/11/Enthalpic-Breakdown.html" rel="alternate" type="text/html" title="Enthalpic Breakdown of Water Structure" /><published>2016-05-11T00:00:00-04:00</published><updated>2016-05-11T00:00:00-04:00</updated><id>http://localhost:4000/papers/2016/05/11/Enthalpic-Breakdown</id><content type="html" xml:base="http://localhost:4000/papers/2016/05/11/Enthalpic-Breakdown.html">&lt;p&gt;In this study, we used simple measures of water structure to characterize active-site solvation using hydration site analysis. We gained valuable insights into the behavior of water under constrined surface topographies of protein active-sites and noted that structural measures, alongside solvation thermodynamics, can be useful in evaluating water displacement for ligand optimization.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Haider K&lt;/strong&gt;, Wickstrom L, Ramsey S, Gilson M, Kurtzman T. Enthalpic breakdown of water structure on protein active-site surfaces. &lt;a href=&quot;http://pubs.acs.org/doi/abs/10.1021/acs.jpcb.6b01094&quot;&gt;J.
Phys. Chem.B. 2016&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="papers" /><summary type="html">In this study, we used simple measures of water structure to characterize active-site solvation using hydration site analysis. We gained valuable insights into the behavior of water under constrined surface topographies of protein active-sites and noted that structural measures, alongside solvation thermodynamics, can be useful in evaluating water displacement for ligand optimization.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22thumb%22=%3E%22water-breakdown.png%22%7D" /></entry><entry><title type="html">Solvation Analysis and Functional Group Mapping of Protein Active-Sites</title><link href="http://localhost:4000/papers/2013/09/26/MCSS-IFST.html" rel="alternate" type="text/html" title="Solvation Analysis and Functional Group Mapping of Protein Active-Sites" /><published>2013-09-26T00:00:00-04:00</published><updated>2013-09-26T00:00:00-04:00</updated><id>http://localhost:4000/papers/2013/09/26/MCSS-IFST</id><content type="html" xml:base="http://localhost:4000/papers/2013/09/26/MCSS-IFST.html">&lt;p&gt;Predicting displacement of water molecules from a target active-site is a worthwhile but challenging endeavor. Here, we combined inhomogeneous solvation theory (IFST) with MCSS to predict water displacement from the HSP90 active-site. While IFST alone performed better than the combined approach in predicting water displacement, the MCSS provided valuable insight into functional groups suitable for displacing the waters.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Haider K&lt;/strong&gt;, Huggins DJ. Combining Solvent Thermodynamic Profiles with Functionality Maps of the Hsp90 Binding Site to Predict
the Displacement of Water Molecules. &lt;a href=&quot;http://pubs.acs.org/doi/abs/10.1021/ci4003409&quot;&gt;J. Chem. Inf. Model. 2013&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="papers" /><summary type="html">Predicting displacement of water molecules from a target active-site is a worthwhile but challenging endeavor. Here, we combined inhomogeneous solvation theory (IFST) with MCSS to predict water displacement from the HSP90 active-site. While IFST alone performed better than the combined approach in predicting water displacement, the MCSS provided valuable insight into functional groups suitable for displacing the waters.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22thumb%22=%3E%22mcss-ifts.png%22%7D" /></entry><entry><title type="html">Modeling Resistance Mutations in Hepatitis C Virus NS3 Protease</title><link href="http://localhost:4000/papers/2013/01/24/NS3.html" rel="alternate" type="text/html" title="Modeling Resistance Mutations in Hepatitis C Virus NS3 Protease" /><published>2013-01-24T00:00:00-05:00</published><updated>2013-01-24T00:00:00-05:00</updated><id>http://localhost:4000/papers/2013/01/24/NS3</id><content type="html" xml:base="http://localhost:4000/papers/2013/01/24/NS3.html">&lt;p&gt;As local HCV patients in Pakistan became increasingly affected by antiviral drug resistance, we set out to model the active-site mutations of NS3 protease, a key anti-HCV drug target. Using implicit-solvent binding energy calculations, we developed a protocol for resistance-profiling of promising clinical candidates. While modeling studies helped us understand the mechanism of resistance, the predictive ability of the approach was hampered potentially by binding-induced structural rearrangements.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hotiana HA, &lt;strong&gt;Haider K&lt;/strong&gt;. Structural Modeling of HCV NS3/4A Serine Protease Drug-Resistance Mutations Using End-Point Continuum
Solvation and Side-Chain Flexibility Calculations. &lt;a href=&quot;http://pubs.acs.org/doi/abs/10.1021/ci3004754&quot;&gt;J. Chem. Inf. Model. 2013.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Kamran Haider</name></author><category term="papers" /><summary type="html">As local HCV patients in Pakistan became increasingly affected by antiviral drug resistance, we set out to model the active-site mutations of NS3 protease, a key anti-HCV drug target. Using implicit-solvent binding energy calculations, we developed a protocol for resistance-profiling of promising clinical candidates. While modeling studies helped us understand the mechanism of resistance, the predictive ability of the approach was hampered potentially by binding-induced structural rearrangements.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22thumb%22=%3E%22ns3.png%22%7D" /></entry></feed>